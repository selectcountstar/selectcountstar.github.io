# SELECT COUNT(\*)

It's a data engineering company.  
Bootstrap data clusters and applications, from single laptop to multi-node.  
Jump start your data experience with our templates.
Fully isolated, DataLabs-in-a-box for your team.

## About
This README defines the mission, vision and offerings of SELECT COUNT(\*)  
Visit http://selectcountstar.github.io

## Approach

Data analytics pipelines are a complex endevour. And as such are a time
a consuming activity both for the corporate departments as well as for the small
and medium enterprise. The complexity is related to four aspects.  

1) Selection of the right set of data analytics components  
2) Configuration of each data component    
3) Integration and setup of the data pipelines  
4) Everything else: CI/CD, Security, DTAP, Logging etc  

We help you by taking care of those aspects. So you can focus on what's really
important for your business.

### Engineering Services

What if someone could lift this work from you and already deliver you the
platform you need? This is exactly what we do at SELECT COUNT(\*). We provide to
you tested, secured and ready-to-use data environments, and guide you in the
process of extracting information and generating ideas fast from your data.

### Partners with Google GCP, Confluent, Elastic, Databricks

Need expertise to fine tune your data architecture to your needs? We have
certified engineers and architects which will help you solving and selecting the
right components for your pipelines. We do this in partnership with Confluent
(kafka), Elastic (elasticsearch, kibana), Databricks (Spark), and Google GCP.

### Databox: a configurable all-in-one datalab-in-a-box

Next to our engineering services, we have designed a web application to help you
configure and deploy a data lab through a friendly UI tour. This app will help
you select the best components and configuration for your specific domain
requirements (retail, telecom, banking, fintech, marketing, logistics)

### Focus on what's important

Simplicity is definitely the ultimate sophistication. We believe that data
analysts should not wait for months before having the chance to dig in the data,
and we believe that data engineers and dev-ops can be be more productive by jump
starting their configuration using our templates and services

## workshops

### "The Ultimate Open Source Data Engineering Demo"
Come and join us for our workshop "The Ultimate Open Source Data Engineering
Demo". Where we put together and end-to-end pipeline for data analytics,
customer segmentation using AI and a reccomender system using jupyter lab,
jupyter notebook, minio, spark, airflow, elasticsearch, logstash, kibana, kafka,
postgres, clickhouse, apache arrow, tensorflow, keras, pytorch, docker and
kubernetes.

Warning this is not a toy demo. It will provide you real insight on how to
build world class data pipelines and predictive APIS. It's real data processing
for a real-world DTAP, CI/CD, and logging for realistic dataset and recommender
application. Limited availability to 20 engineers and managers every week.

Dates:

### The Ultimate Data Team.
How do you build an effective data team? How you make sure it works fine and has
the skills to run full data analytics pipelines from reporting to predicting
APIs? What are the pitfalls and how to avoid them?  


## Consultancy

Next to the databox offering, we are a group of passionate software engineers,
devops, and data scientist. Happy to help you with the following headaches: Data
Ingestion, Data Modeling, Data Science, Data Engineering, Data Integration at
your premises.

We also offer consultancy on Spark, Airflow, Cassandra, Redis, Min.io,
ElasticSearch, Logstash, Kibana, Kafka. Need help? Call us now and we will bring
the expertise you need to configure those components correctly and build
bespoken application and use-cases with it.

Fancy migrating your Data Architecture to the cloud?
We are Google GCP partners, and part of the Google Developer Experience program.

Don't know how to setup your streaming analytics for marketing rewards or your
real-time fraud detection engine? We are Confluent partners and experience with
Streaming SQL solutions.

## Products
### databox

Databox is a template generator to produced fully functional and configurable
data science experiences. Just select the components you want and generate your
Data Lab on your favorite setup no matter if cloud or metal.

The databox, are fully air-gapped, and isolated. Multiple security layers are in
place, an already configured clientless web remote desktop, and resources
protected with RBAC, LDAP, OAuth2, and a secret Vault.

Data is encrypted at rest, and connectivity limited to a web remote desktop to
guarantee the maximum confidentiality and security on the data being analyzed.
You can decide to your own user collaboration strategy, such as isolated,
federated or shared data patterns.

Databox is ideal for both data science experiments as well as production
workflows. The web app will generate the right template for you depending on
your typical Data Applications such as Ingestion, ETL, Reporting, EDA, Data
Science.

Multiple domain specific visualization apps are already available for your
advanced analytics use cases, such as recommenders, classifiers, anomaly
detectors, and data clustering. Both available as APIs, Notebooks, and Web Apps
to serve and boost analytics for everyone, from the business analyst to the data
scientist.

Very rich Data Exploratory Environment, with all the main packages, storage,
processing engines and web application already pre-installed to jump start your
data analysis and produce results and insight from day one.

DTAP ready so you can test, develop and stage your data pipelines without
spending hours setting up the CI/CD environment and code multiple
configurations. Logging and monitoring is also part of the databox with an
already configured kafka bus to log all steps in your data processing pipeline.

Databox will setup for you the following components: jupyter lab, jupyter
notebook, hdfs, minio, spark, airflow, elasticsearch, logstash, kibana, kafka,
and a variety of databases such as postgres, mysql for transactional workload
and clickhouse for OLAP analytics.

Databox let you mix and match Data Preparation, ETL, Machine Learning and AI, so
that you can control your data pipeline as an end-to-end process. Moreover we
preconfigure for you, tensorflow, keras, pythorch, scikit-learn as part of a
selected list of over 100 packages to serve the most experienced data scientist
needs.

Databox "Metal"  
Databox "Rack"  
Databox "Laptop"  
Databox "Cloud"  

### Dataloof

Dataloof a high productivity open-source python package for data engineering
which will seriously speed up the coding of your data pipelines. Here is a list
of high-level benefits you gain by using this package:

#### For Data Ops:
Quickly instantiate effective and tested data engineering
templates for Data Ingestion, ETL, Data Preparation, Data Science, etc.

Access and use seemlessly multiple data computing engines (spark, dask, cudf,
flink, pandas), using the same programming style for data processing and data
preparation.

Connect to multiple data storage services, databases, object stores with ease,
just by defining a few connectivity parameters.

Define all your data resources, data service providers, configuration and
run-time variables, DTAP profiles, engine and logging configuration with yaml
files.

Provide a simple and intuitive way to log your data pipelines via mutliple
channels such as files, syslog, kafka, mlflow, tensorboard.

#### For Data Engineers:

Simplify your data ingestion, and code data ingestion including logging, and
data quality checks in just a few lines of code.

Provide a simple way to load, transform and save data, using multiple formats,
encoding and compression techniques. Add data transformations, such as
filtering, feature engineering, anomaly detection and clustering with just a few lines
of code.

Explore data visually using a number of very rich visualization (facets, column
statistics, embeddings) directly via a webapp or via directly in your jupyter
notebook.

Perform data aggregation and user/customer/product profiling in just a few lines
of code. Perfom clustering and Exploratory Data Analysis on profiles, to
validate your business intuitions.

#### For ML engineers:

Provide a simple way to extra clusters, segments, anomalies from your data.
Quickly generate base models for classification, forecasting, recommenders.

Save the results either as a model (for dynamic predictive APIs) or as a
materialized table (for fast access on postgres, elastic, redis).

## Solutions

Solutions Our solutions are flexible and can be applied in a number of domains.
Please contact us for any demo!

### Data Engineering
#### Data Ingestion

Simple to configure and template-based ingestion, including Reliability
Engineering practices, such as data quality monitoring, ingestion continuous
logging, and DTAP profiles. We have a solution to quickly generate high quality
and reproducable ingestion pipelines fully tested with CI/CD configuration
scripts for gitlab CI/CD and airflow.

#### ETL and BI



#### Data Publishing

### Machine Learning
#### Sales Forecasting
#### Product Basket and Browsing Analysis

### Artificial Intelligence
#### Customer&Product Segmentation
#### Content/Collaborative Recommender System
#### Multi-Objective Marketing Campaign

### Streaming Computing
#### Anomaly Detection
#### Real-time BI Analytics

# Partnerships
We are part of a vibrant Open Source and Open Core ecosystem.  
Currently, on going partnerships with:

 - Google Cloud (GCP program)
 - Confluent (Kafka, KSQL)
 - Elastic (Elasticsearch, Kibana)
 - Databricks (Spark)

## Meet the team

Founders

Natalino Busa
Hien Mai

Investors
VN Life Singapore

## We are Hiring!

### Data Dev-Ops
### Data Pipeline Engineers
### Data Streaming Engineers
### API Developers
### Front-End Developers
