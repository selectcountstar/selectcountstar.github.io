# SELECT COUNT(\*)

It's a data engineering company.  
We help you bootstrap and mantain your data clusters and applications, from single laptop to multi-node.  

Jump start your data experience with our templates.
Fully isolated, DataLabs-in-a-box for your team.

## About

This README defines the mission, vision and offerings of SELECT COUNT(\*)  
Visit <http://selectcountstar.github.io>

## Approach

Data analytics pipelines are a complex endevour. And as such are a time
a consuming activity both for the corporate departments as well as for the small
and medium enterprise. The complexity is related to four aspects.  

1) Selection of the right set of data analytics components  
2) Configuration of each data component  
3) Integration and setup of the data pipelines  
4) Everything else: CI/CD, Security, DTAP, Logging etc  

We help you by taking care of those aspects.  
So you can focus on what's really important for your business.

### Engineering Services

What if someone could lift this work from you and already deliver you the
platform you need? This is exactly what we do at SELECT COUNT(\*). We provide to
you tested, secured and ready-to-use data environments, and guide you in the
process of extracting information and generating ideas fast from your data.

### Partners with Google GCP, Confluent, Elastic, Databricks

Need expertise to fine tune your data architecture to your needs? We have
certified engineers and architects which will help you solving and selecting the
right components for your pipelines. We do this in partnership with Confluent
(kafka), Elastic (elasticsearch, kibana), Databricks (Spark), and Google GCP.

### Databox: a configurable all-in-one datalab-in-a-box

Next to our engineering services, we have designed a web application to help you
configure and deploy a data lab through a friendly UI tour. This app will help
you select the best components and configuration for your specific domain
requirements (retail, telecom, banking, fintech, marketing, logistics)

### Focus on what's important
Dive into the data analytics straight away. Don't wait up months
before having your data engineering sorted out. Jump start your data clusters
using our data templates and data engineering services.

## workshops

### "The Ultimate Open Source Data Engineering Demo"

Come and join us for our workshop "The Ultimate Open Source Data Engineering
Demo". Where we put together and end-to-end pipeline for data analytics,
customer segmentation using AI and a reccomender system using jupyter lab,
jupyter notebook, minio, spark, airflow, elasticsearch, logstash, kibana, kafka,
postgres, clickhouse, apache arrow, tensorflow, keras, pytorch, docker and
kubernetes.

Warning this is not a toy demo. It will provide you real insight on how to
build world class data pipelines and predictive APIS. It's real data processing
for a real-world DTAP, CI/CD, and logging for realistic dataset and recommender
application. Limited availability to 20 engineers and managers every week.

Dates:

### The Ultimate Data Team.

How do you build an effective data team? How you make sure it works fine and has
the skills to run full data analytics pipelines from reporting to predicting
APIs? What are the pitfalls and how to avoid them?  

## Consultancy

Next to the databox offering, we are a group of passionate software engineers,
devops, and data scientist. Happy to help you with the following headaches: Data
Ingestion, Data Modeling, Data Science, Data Engineering, Data Integration at
your premises.

We also offer consultancy on Spark, Airflow, Cassandra, Redis, Min.io,
ElasticSearch, Logstash, Kibana, Kafka. Need help? Call us now and we will bring
the expertise you need to configure those components correctly and build
bespoken application and use-cases with it.

Fancy migrating your Data Architecture to the cloud?
We are Google GCP partners, and part of the Google Developer Experience program.

Don't know how to setup your streaming analytics for marketing rewards or your
real-time fraud detection engine? We are Confluent partners and experience with
Streaming SQL solutions.

## Products

### databox

Databox is a template generator to produced fully functional and configurable
data science experiences. Just select the components you want and generate your
Data Lab on your favorite setup no matter if cloud or metal.

The databox, are fully air-gapped, and isolated. Multiple security layers are in
place, an already configured clientless web remote desktop, and resources
protected with RBAC, LDAP, OAuth2, and a secret Vault.

Data is encrypted at rest, and connectivity limited to a web remote desktop to
guarantee the maximum confidentiality and security on the data being analyzed.
You can decide to your own user collaboration strategy, such as isolated,
federated or shared data patterns.

Databox is ideal for both data science experiments as well as production
workflows. The web app will generate the right template for you depending on
your typical Data Applications such as Ingestion, ETL, Reporting, EDA, Data
Science.

Multiple domain specific visualization apps are already available for your
advanced analytics use cases, such as recommenders, classifiers, anomaly
detectors, and data clustering. Both available as APIs, Notebooks, and Web Apps
to serve and boost analytics for everyone, from the business analyst to the data
scientist.

Very rich Data Exploratory Environment, with all the main packages, storage,
processing engines and web application already pre-installed to jump start your
data analysis and produce results and insight from day one.

DTAP ready so you can test, develop and stage your data pipelines without
spending hours setting up the CI/CD environment and code multiple
configurations. Logging and monitoring is also part of the databox with an
already configured kafka bus to log all steps in your data processing pipeline.

Databox will setup for you the following components: jupyter lab, jupyter
notebook, hdfs, minio, spark, airflow, elasticsearch, logstash, kibana, kafka,
and a variety of databases such as postgres, mysql for transactional workload
and clickhouse for OLAP analytics.

Databox let you mix and match Data Preparation, ETL, Machine Learning and AI, so
that you can control your data pipeline as an end-to-end process. Moreover we
preconfigure for you, tensorflow, keras, pythorch, scikit-learn as part of a
selected list of over 100 packages to serve the most experienced data scientist
needs.

Databox "Metal"  
Databox "Rack"  
Databox "Laptop"  
Databox "Cloud"  

### Datafaucet

Our open-sourced python library datafaucet is a high productivity collection of
data engineering tools which will seriously speed up the coding of your data
pipelines. Here is a list of high-level benefits you gain by using this package:

#### For Data Ops:

Quickly instantiate effective and tested data engineering
templates for Data Ingestion, ETL, Data Preparation, Data Science, etc.

Access and use seemlessly multiple data computing engines (spark, dask, cudf,
flink, pandas), using the same programming style for data processing and data
preparation.

Connect to multiple data storage services, databases, object stores with ease,
just by defining a few connectivity parameters.

Define all your data resources, data service providers, configuration and
run-time variables, DTAP profiles, engine and logging configuration with yaml
files.

Provide a simple and intuitive way to log your data pipelines via mutliple
channels such as files, syslog, kafka, mlflow, tensorboard.

#### For Data Engineers:

Simplify your data ingestion, and code data ingestion including logging, and
data quality checks in just a few lines of code.

Provide a simple way to load, transform and save data, using multiple formats,
encoding and compression techniques. Add data transformations, such as
filtering, feature engineering, anomaly detection and clustering with just a few lines
of code.

Explore data visually using a number of very rich visualization (facets, column
statistics, embeddings) directly via a webapp or via directly in your jupyter
notebook.

Perform data aggregation and user/customer/product profiling in just a few lines
of code. Perfom clustering and Exploratory Data Analysis on profiles, to
validate your business intuitions.

#### For ML engineers:

Provide a simple way to extra clusters, segments, anomalies from your data.
Quickly generate base models for classification, forecasting, recommenders.

Save the results either as a model (for dynamic predictive APIs) or as a
materialized table (for fast access on postgres, elastic, redis).

## Solutions

Our solutions are flexible and can be applied in a number of domains.  
Please contact us for any demo!

### Data Engineering

#### Data Ingestion

Simple to configure and template-based ingestion, including Reliability
Engineering practices, such as data quality monitoring, ingestion continuous
logging, and DTAP profiles. We have a solution to quickly generate high quality
and reproducable ingestion pipelines fully tested with CI/CD configuration
scripts for gitlab CI/CD and airflow.

#### ETL and BI

We have templates to speed up the creation of your star schema. And automated
methods to join your tables, including the elusive Slow Changing Dimension
patterns. Help generating highly efficient representation from your data sources
running a AI and ML powered table analysis, and suggesting efficient table join
sequences.

#### Data Publishing

Save Fact and Dimension table in format which can be quickly and efficiently
consumed by your BI applications. Provide security and Role and Resource Bases
Access Control for your published Fact tables and Star Schema

Provide template to publish and validate saved data on a variety of target
platforms such as ClickHouse, Elastic, Postgres as well as bespoken OLAP cloud
services such as BigQuery, and SnowFlake.

### Machine Learning

#### Sales Forecasting

We have build a flexible and customizable RESTfull API service to produce high
quality timeserie forecasting for your sales, marketing, inventory, visit
budgeting needs.  

#### Product Basket and Browsing Analysis

Sophisticated re-targetable data analytics solution to explore graph data.
Another great template which is ready to use to analyze popularity and most
common selected products or visited pages, using ML graph analytical techniques
such as PageRank and FPGrowth.

### Artificial Intelligence

#### Customer&Product Segmentation

We have a ready-to-use AI API and Web App for Customer and Product segmentation
based to a novel autoembedding engine written in tensorflow. Call us to know
more about this great demo!

#### Content/Collaborative Recommender System

Recommeder Systems are probably one of the most important application of machine
learning for Retail, but also for many other domains such as Marketing, HR,
Sales, Promotional Campaigns etc.

We have built many recommenders using both traditional ML and Statistic
techniques as well as models using very sophisticated Natural Language
Processing AI engines. Interested? Book a demo with us!

#### Multi-Objective Marketing Campaign

You have probably have heard of A/B testing. But what if you have 10s or 100s of
different marketing strategies? How are you going to optimally select those
which are more effective and still keep exploring the others?

We are experienced with Reinforcement Learning Techniques for managing a large
number of diverse (even conflicting) marketing recommendation strategies. Please
take contact with us understand more about this demo.

### Streaming Computing

#### Real-time BI Analytics

More and more often business decision must be taken fast rather than waiting on
reporting with days or week long cycles. We have extensive expertise in building
streaming analytics solutions in a varienty of platforms (Spark, Kafka, Elastic,
Flink/Beam)

#### Outlier Detection

Two use cases:

-   Catch cheating/fraud in your service usage or in your product sales.  
-   Ride promotions and adjust rates and discounts for burst sales in your products  

For both this use cases you need streaming analytics and in particular outlier
detection. These solutions range from simple statistical analysis to
sophisticated deep learning models. Let us show you a demo on how to build a
robust streaming outlier processing pipeline.

## Partnerships

We are part of a vibrant Open Source and Open Core ecosystem.  
Currently, on going partnerships with:

-   Google Cloud (GCP program)
-   Confluent (Kafka, KSQL)
-   Elastic (Elasticsearch, Kibana)
-   Databricks (Spark)

## Call for action!

### We like to solve problems

Are you managing a data team and need help?
Contact us! We are available, ready to tackle your ideas!

### We are hiring!

Are you a passionate and telented software engineer?  
Join us! Not a dull day, we promise!  

#### Data Dev-Ops

#### Data Pipeline Engineers

#### Data Streaming Engineers

#### API Developers

#### Front-End Developers
